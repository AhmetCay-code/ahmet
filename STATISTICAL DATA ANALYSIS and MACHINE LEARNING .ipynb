{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION AND SUMMARY\n",
    "\n",
    "Machine learning models are based on statistics, optimization and computer science. Our project aims to compare the performance of a few machine learning models on different databases. For this analysis, we used the Python programming language on Jupyter notebooks. The project will be realized in 2 terms and we conducted the experimental part in our first period. We will present the theoretical discussion of mathematical basis of the models use in the fall period of 2021.\n",
    "\n",
    "## PROJECT PLAN\n",
    "\n",
    "### Aim of the Project\n",
    "\n",
    "We aim to perform rigorous statistical analysis of 6 different machine learning models on 3 different datasets.\n",
    "\n",
    "### Scope of the Project\n",
    "\n",
    "Within the scope of this project, 6 different machine learning models were selected from well-known and commonly used python libraries. A deeper theoretical exploration about the models will be given at the thesis stage. Since we have devoted our first period to experiments, we will show an explanatory code section of the project in this report.\n",
    "\n",
    "### Area of Usage\n",
    "\n",
    "Within the scope of the analysis, the final version of the project can be used as a resource for those who want to learn the mathematical background of machine learning models, and how they can be applied to real-world data sets. We aim that the thesis will be a resource for those who want to learn from the beginning which models work best for which dataset.\n",
    "\n",
    "### Schedule\n",
    "\n",
    "| Task | Alloted Time |\n",
    "| ---  | ---          |  \n",
    "| Research and discovery of databases | 2 weeks |\n",
    "| Clearing and preparing datasets     | 2 weeks |\n",
    "| Determining the models and libraries to be used | 1 week |\n",
    "| Coding phase and evaluation of outputs | 10 weeks |\n",
    "| General arrangement of the code stage and annotation | 2 weeks |\n",
    "| Writing the report | 3 weeks |\n",
    "\n",
    "\n",
    "### Resources\n",
    "\n",
    "All databases, libraries and models used are open source. All relevant references will be made to the books, articles and theses used for theory in the reference section in the thesis section. All work is stored on github. ( https://github.com/Code-Cash/ahmet )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXPERIMENTS\n",
    "\n",
    "Our project is based on a through and rigorous statistical analysis of 6 different machine learning algorithms over 3 databases.  \n",
    "\n",
    "The code block you see below is the experimental stage of this thesis. We give descriptions about each cell just above the corresponding cell.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the libraries\n",
    "\n",
    "At the beginning of our project, we need to add the libraries required for our project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the models\n",
    "\n",
    "We load 6 blank machine learning models after adding our libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs = -1)\n",
    "logreg = LogisticRegression(solver='liblinear',multi_class='ovr')\n",
    "svm = SVC(gamma='auto')\n",
    "xgb = xgb.XGBClassifier()\n",
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the first data set\n",
    "\n",
    "We add our first data set using the pandas library. Then we display the first 5 elements of our dataset using the panda's head function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MUSK</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MUSK</td>\n",
       "      <td>46</td>\n",
       "      <td>-108</td>\n",
       "      <td>-60</td>\n",
       "      <td>-69</td>\n",
       "      <td>-117</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>-161</td>\n",
       "      <td>-8</td>\n",
       "      <td>...</td>\n",
       "      <td>-308</td>\n",
       "      <td>52</td>\n",
       "      <td>-7</td>\n",
       "      <td>39</td>\n",
       "      <td>126</td>\n",
       "      <td>156</td>\n",
       "      <td>-50</td>\n",
       "      <td>-112</td>\n",
       "      <td>96</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MUSK</td>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-6</td>\n",
       "      <td>57</td>\n",
       "      <td>-171</td>\n",
       "      <td>-39</td>\n",
       "      <td>...</td>\n",
       "      <td>-59</td>\n",
       "      <td>-2</td>\n",
       "      <td>52</td>\n",
       "      <td>103</td>\n",
       "      <td>136</td>\n",
       "      <td>169</td>\n",
       "      <td>-61</td>\n",
       "      <td>-136</td>\n",
       "      <td>79</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MUSK</td>\n",
       "      <td>46</td>\n",
       "      <td>-194</td>\n",
       "      <td>-145</td>\n",
       "      <td>28</td>\n",
       "      <td>-117</td>\n",
       "      <td>73</td>\n",
       "      <td>57</td>\n",
       "      <td>-168</td>\n",
       "      <td>-39</td>\n",
       "      <td>...</td>\n",
       "      <td>-134</td>\n",
       "      <td>-154</td>\n",
       "      <td>57</td>\n",
       "      <td>143</td>\n",
       "      <td>142</td>\n",
       "      <td>165</td>\n",
       "      <td>-67</td>\n",
       "      <td>-145</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MUSK</td>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>-170</td>\n",
       "      <td>-39</td>\n",
       "      <td>...</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>136</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MUSK</td>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>-170</td>\n",
       "      <td>-39</td>\n",
       "      <td>...</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>137</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MUSK   2    3    4   5    6   7   8    9  10  ...  159  160  161  162  163  \\\n",
       "0  MUSK  46 -108  -60 -69 -117  49  38 -161  -8  ... -308   52   -7   39  126   \n",
       "1  MUSK  41 -188 -145  22 -117  -6  57 -171 -39  ...  -59   -2   52  103  136   \n",
       "2  MUSK  46 -194 -145  28 -117  73  57 -168 -39  ... -134 -154   57  143  142   \n",
       "3  MUSK  41 -188 -145  22 -117  -7  57 -170 -39  ...  -60   -4   52  104  136   \n",
       "4  MUSK  41 -188 -145  22 -117  -7  57 -170 -39  ...  -60   -4   52  104  137   \n",
       "\n",
       "   164  165  166  167  168  \n",
       "0  156  -50 -112   96  1.0  \n",
       "1  169  -61 -136   79  1.0  \n",
       "2  165  -67 -145   39  1.0  \n",
       "3  168  -60 -135   80  1.0  \n",
       "4  168  -60 -135   80  1.0  \n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database1 = pd.read_csv(\"data.csv\", sep=';')\n",
    "database1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection and splitting the train and test data sets\n",
    "\n",
    "Let us denote the features we are going to use with `X` and response variable with `y`. With the drop function, we get the columns other than the columns 'musk' and '168'. For the response variable, we just take the column named '168'. Then we determine the size of our training and test sets. For this dataset, 90% of all data is used for training and the remaining 10% is used for testing using the function \"train_test_split\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = database1.drop([\"MUSK\",\"168\"],axis=1)\n",
    "y = database1['168']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A generic function for training and evaluating a model\n",
    "\n",
    "At this stage, we create a function that will run our 6 algorithms for our first dataset and give us 3 outputs. \n",
    "\n",
    "If we go into detail: first, we train our model with the features `X`, which is the training group we mentioned in Section 4, with the features in `y` as dependent variable. Then we make a prediction with the predict function according to the test data set. We record this prediction as `y_pred`. The test data set `y_test` is the test set of the results will give us the accuracy of the model. Then we print the classification report and confusion matrix with the help of the scikit-learn library. The explanation of all these functions will be found in the theory section of our thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(name, X_train, y_train, X_test, y_test):\n",
    "    name.fit(X_train, y_train)\n",
    "    y_pred = name.predict(X_test)\n",
    "    score= accuracy_score(y_test, y_pred) * 100\n",
    "    print(str(name) + \"Accuracy:\",score)\n",
    "    report_name=classification_report(y_test, y_pred)\n",
    "    print(report_name)\n",
    "    print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination\n",
    "\n",
    "The purpose of the function below is to find the most suitable features for us in our database. The `rfe` (Recursive Feature Elimination) function produces an output based on the model we want and the number of desired features. It shows them in order, according to their effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfe(model,X,y):  \n",
    "    rfe = RFE(model, 16)\n",
    "    fit = rfe.fit(X, y)\n",
    "    print(\"Num Features: %s\" % (fit.n_features_))\n",
    "    print(\"Selected Features: %s\" % (fit.support_))\n",
    "    print(\"Feature Ranking: %s\" % (fit.ranking_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Cross validation calculates the average accuracy and standard deviation for each part by dividing our data into as many parts as we want. This gives us an important idea of how well the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossval(model,X,y,n=10):\n",
    "    scores=cross_val_score(model, X, y, cv=n, scoring =\"accuracy\")\n",
    "    print(str(model) + \"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on the first data set\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "Let us evaluate the Decision Tree model on our first data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')Accuracy: 96.21212121212122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.98      0.98       549\n",
      "         1.0       0.90      0.87      0.89       111\n",
      "\n",
      "    accuracy                           0.96       660\n",
      "   macro avg       0.94      0.93      0.93       660\n",
      "weighted avg       0.96      0.96      0.96       660\n",
      "\n",
      "[[538  11]\n",
      " [ 14  97]]\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')Accuracy: 0.78 (+/- 0.40)\n",
      "Num Features: 16\n",
      "Selected Features: [False False False False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True False False False  True\n",
      " False  True False False False False  True False False False False False\n",
      " False  True False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False  True False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True False  True False  True False False False False False  True\n",
      " False False False False False False False  True False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False  True False False False]\n",
      "Feature Ranking: [146 145  45  62  46  32  66  61   1  53  71  70  75  60  78  40   9  69\n",
      "  83  15  93  12  90  88  24 103   8 108  43 110  85   1 122  41  34   1\n",
      "  54   1  33  74  80  18   1 136 141  86 134  36  56   1 139   3  68   2\n",
      "   6  37  95  98  92  58  38  82  21  26 140   1 117  50  35 118 128 100\n",
      " 144 130  17 143   1 113 119 123 121 149   1 142   4 138  89  31   5 137\n",
      "  77 106  14  59  84  16  91  76  11  73  25  65  27  63 109  10  81  28\n",
      "  64 107  51  67 120 105 101  30  22  96 114  52  29   1  97   1  99   1\n",
      " 127 147 129  47  49   1   7 125 124  44 102  94 111   1  19  48  39 135\n",
      " 132 133  87 126 115 131   1  23  72 116 104  42 112  13  79  55  57  20\n",
      "   1 148 150 151]\n"
     ]
    }
   ],
   "source": [
    "model(dt,X_train,y_train,X_test,y_test)\n",
    "crossval(dt,X,y,n)\n",
    "rfe(dt,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Let us evaluate the Random Forest algorithm on our first data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=-1, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)Accuracy: 97.12121212121212\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.98       549\n",
      "         1.0       0.99      0.84      0.91       111\n",
      "\n",
      "    accuracy                           0.97       660\n",
      "   macro avg       0.98      0.92      0.95       660\n",
      "weighted avg       0.97      0.97      0.97       660\n",
      "\n",
      "[[548   1]\n",
      " [ 18  93]]\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=-1, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)Accuracy: 0.80 (+/- 0.41)\n",
      "Num Features: 16\n",
      "Selected Features: [False False False False False False False False  True False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False  True False  True False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False  True False False False False False False False False False False\n",
      " False  True False  True False  True False False False False False  True\n",
      " False False False False False False False  True False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False  True  True False False False]\n",
      "Feature Ranking: [  7 143  50  72 150  77  87  83   1  20 110 136  45  19 128 145  28 123\n",
      "  38  44   2   8  58 121  17  95 131 134  94  96 106  25  43  53  24   1\n",
      " 140  40  74 130  76  29   5  69   4 125 144  68  86   1  49   1  56  98\n",
      "  32  59 147  90  42 105  10  97   9  92 116   1 151  47 138 113 108  31\n",
      " 122 126  62 149  91  82  23  39  89  22  12  75  11  16 139 119  26 114\n",
      " 127   1  61  81   1  27 120  60  71 141 115  13  55  63  35  93  73  46\n",
      "  15   1  66  30 102  99  37  36  65  54 135 112  84   1  52   1  41   1\n",
      " 103  57  78  64  51   1  18  80  67 129 137 101 104   1  21 132  70  33\n",
      " 142  88 148 118 117  79   1 124  85 100 107 111 146 109 133   3  48   1\n",
      "   1  34   6  14]\n"
     ]
    }
   ],
   "source": [
    "model(rf,X_train,y_train,X_test,y_test)\n",
    "crossval(rf,X,y)\n",
    "rfe(rf,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Let us evaluate the Logistic Regression algorithm on our first data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)Accuracy: 95.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.99      0.97       549\n",
      "         1.0       0.92      0.77      0.84       111\n",
      "\n",
      "    accuracy                           0.95       660\n",
      "   macro avg       0.94      0.88      0.90       660\n",
      "weighted avg       0.95      0.95      0.95       660\n",
      "\n",
      "[[542   7]\n",
      " [ 26  85]]\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)Accuracy: 0.81 (+/- 0.28)\n",
      "Num Features: 16\n",
      "Selected Features: [False False False False False False False False False False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False  True False  True False False False False False\n",
      " False False False False False False False False  True  True False False\n",
      " False False False False False False  True False False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False  True False False False False False False False False False\n",
      " False False False False False False False False False False  True False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False  True False False False False False False\n",
      " False False False False  True False False False False False False False\n",
      " False False False  True False  True  True False False False]\n",
      "Feature Ranking: [ 16  48  14  73 145  60 146  95  13  10  71  35   1  62  63  65   4 122\n",
      " 142 140 138  12 128 137  67  88   1 101  55 125  49  43  17 130  25  39\n",
      " 100  19  38  31   1  41   1 143  85 147  46  78  94  21  80   6  76 149\n",
      " 148  91   1   1  57 106 117 123 114 108  27  18   1 102 141  32  15   1\n",
      "  70  84  87  82 124  64  93 139  75 121  81 116   9  45   1  92  97  59\n",
      " 134  51  54  69 151   2 133   5   1 103  40  52  74  86  61 127  83 104\n",
      " 111 112 110 129  77  29 105 107  72  90   1  11  66  56  58  98   3  50\n",
      "   8 126   7  26  28 119  79  36  96  20  47   1  33  34 109 150  68  23\n",
      " 144 120  37 118   1  30  24  53 136  99  42  44 135 113 131   1 132   1\n",
      "   1  22  89 115]\n"
     ]
    }
   ],
   "source": [
    "model(logreg,X_train,y_train,X_test,y_test)\n",
    "crossval(logreg,X,y)\n",
    "rfe(logreg,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)\n",
    "\n",
    "Let us evaluate the Support Vector Machines algorithm on our first data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)Accuracy: 88.63636363636364\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      1.00      0.94       549\n",
      "         1.0       1.00      0.32      0.49       111\n",
      "\n",
      "    accuracy                           0.89       660\n",
      "   macro avg       0.94      0.66      0.71       660\n",
      "weighted avg       0.90      0.89      0.86       660\n",
      "\n",
      "[[549   0]\n",
      " [ 75  36]]\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)Accuracy: 0.85 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "model(svm,X_train,y_train,X_test,y_test)\n",
    "crossval(svm,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost \n",
    "\n",
    "Let us evaluate the XGBoost algorithm on our first data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
      "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)Accuracy: 99.0909090909091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99       549\n",
      "         1.0       0.99      0.95      0.97       111\n",
      "\n",
      "    accuracy                           0.99       660\n",
      "   macro avg       0.99      0.98      0.98       660\n",
      "weighted avg       0.99      0.99      0.99       660\n",
      "\n",
      "[[548   1]\n",
      " [  5 106]]\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
      "              objective='binary:logistic', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)Accuracy: 0.86 (+/- 0.28)\n",
      "Num Features: 16\n",
      "Selected Features: [ True False False False False False False False  True False False False\n",
      " False False False False False False False False False  True False False\n",
      " False False False False False False False  True False False  True  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True False False  True False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True False False False False False False False False False False\n",
      " False False False  True False  True False False False False False  True\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False  True False  True False]\n",
      "Feature Ranking: [  1  47  51  66  50  49  92 125   1   3 116 113 102  99  87  95   6  70\n",
      "  80 121  10   1  75 109  53 150  31  81  77  63  18   1  67  40   1   1\n",
      " 111   4 119  38 148  15  29  74 127 136 128  82  94  16  62  21  84  73\n",
      "   8  46 103  48  36 142  34  11   1 132 135   1  27  39  90  93 147 122\n",
      " 120 104  97 105   9 139 133  20  85 112  26  98 144  43 151  35  23 118\n",
      "  56   1  19 107 114  54 134  88  78 124  24  37  57  89  65  79 106 110\n",
      "  55   1  32 108 117  33  52  41  44  71  91  72  64  14  58   1  83   1\n",
      " 140 126 129  69   5   1  17 123 115  42  28  61 130   2  59 145  96  45\n",
      "  25  13 146  60 143 149   1 141  12  76  30   7 138 101 100  86 131  22\n",
      "   1  68   1 137]\n"
     ]
    }
   ],
   "source": [
    "model(xgb,X_train,y_train,X_test,y_test)\n",
    "crossval(xgb,X,y)\n",
    "rfe(xgb,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "\n",
    "Let us evaluate the Gaussian Naive Bayes algorithm on our first data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None, var_smoothing=1e-09)Accuracy: 83.03030303030303\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.84      0.89       560\n",
      "         1.0       0.46      0.76      0.58       100\n",
      "\n",
      "    accuracy                           0.83       660\n",
      "   macro avg       0.71      0.80      0.73       660\n",
      "weighted avg       0.88      0.83      0.85       660\n",
      "\n",
      "[[472  88]\n",
      " [ 24  76]]\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)Accuracy: 0.80 (+/- 0.31)\n"
     ]
    }
   ],
   "source": [
    "model(gnb,X_train,y_train,X_test,y_test)\n",
    "crossval(gnb,X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on Our Second Data Set\n",
    "\n",
    "Our second dataset is slightly different from the first one. The training and test sets consist of two different csv blocks, so we define these two datasets separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>21</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "      <td>92</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>-5</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>40</td>\n",
       "      <td>48</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>-26</td>\n",
       "      <td>43</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f1  f2  f3  f4  f5  f6  f7  f8  f9  d\n",
       "0  50  21  77   0  28   0  27  48  22  2\n",
       "1  55   0  92   0   0  26  36  92  56  4\n",
       "2  53   0  82   0  52  -5  29  30   2  1\n",
       "3  37   0  76   0  28  18  40  48   8  1\n",
       "4  37   0  79   0  34 -26  43  46   2  1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database2 = pd.read_csv(\"shuttle-train.csv\", sep=';')\n",
    "database2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>d</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>-6</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>88</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>-4</td>\n",
       "      <td>40</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>-1</td>\n",
       "      <td>89</td>\n",
       "      <td>-7</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>9</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>-2</td>\n",
       "      <td>25</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>-6</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f1  f2  f3  f4  f5  f6  f7  f8  f9  d\n",
       "0  55   0  81   0  -6  11  25  88  64  4\n",
       "1  56   0  96   0  52  -4  40  44   4  4\n",
       "2  50  -1  89  -7  50   0  39  40   2  1\n",
       "3  53   9  79   0  42  -2  25  37  12  4\n",
       "4  55   2  82   0  54  -6  26  28   2  1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database3 = pd.read_csv(\"shuttle-test.csv\", sep=';')\n",
    "database3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test Split\n",
    "\n",
    "At this stage, we already have separate test and train subsets, and as a result, we do not need to use the `train_test_split` function we uses for the first dataset. Because currently our training and test sets are already given. All we have to do is to assign the independent and dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train = database2.drop([\"d\"], axis=1)\n",
    "X1_test = database3.drop([\"d\"], axis=1)\n",
    "y1_train = database2['d']\n",
    "y1_test = database3['d']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Train and Test for Cross-Validation\n",
    "\n",
    "This stage is actually combining the operations we did in stage 9, the two datasets above and then assigning the properties as df_row_reindex_X and the results as df_row_reindex_y. As a result, it allows us to work with a single dataset, not two. We do this with the function named concat in the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_row_reindex = pd.concat([database2, database3], ignore_index=True)\n",
    "df_row_reindex_X = df_row_reindex.drop([\"d\"], axis=1)\n",
    "df_row_reindex_y = df_row_reindex['d']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "Let us evaluate the Decision Tree algorithm on our second data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')Accuracy: 99.99310344827586\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00     11478\n",
      "           2       1.00      0.92      0.96        13\n",
      "           3       1.00      1.00      1.00        39\n",
      "           4       1.00      1.00      1.00      2155\n",
      "           5       1.00      1.00      1.00       809\n",
      "           6       1.00      1.00      1.00         4\n",
      "           7       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00     14500\n",
      "   macro avg       1.00      0.99      0.99     14500\n",
      "weighted avg       1.00      1.00      1.00     14500\n",
      "\n",
      "[[11478     0     0     0     0     0     0]\n",
      " [    0    12     0     1     0     0     0]\n",
      " [    0     0    39     0     0     0     0]\n",
      " [    0     0     0  2155     0     0     0]\n",
      " [    0     0     0     0   809     0     0]\n",
      " [    0     0     0     0     0     4     0]\n",
      " [    0     0     0     0     0     0     2]]\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')Accuracy: 1.00 (+/- 0.00)\n",
      "Num Features: 9\n",
      "Selected Features: [ True  True  True  True  True  True  True  True  True]\n",
      "Feature Ranking: [1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "model(dt,X1_train,y1_train,X1_test,y1_test)\n",
    "crossval(dt,df_row_reindex_X,df_row_reindex_y)\n",
    "rfe(dt,df_row_reindex_X,df_row_reindex_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Let us evaluate the Random Forest algorithm on our second data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=-1, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)Accuracy: 99.97931034482758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00     11478\n",
      "           2       1.00      0.92      0.96        13\n",
      "           3       0.97      1.00      0.99        39\n",
      "           4       1.00      1.00      1.00      2155\n",
      "           5       1.00      1.00      1.00       809\n",
      "           6       1.00      0.75      0.86         4\n",
      "           7       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           1.00     14500\n",
      "   macro avg       1.00      0.88      0.92     14500\n",
      "weighted avg       1.00      1.00      1.00     14500\n",
      "\n",
      "[[11478     0     0     0     0     0     0]\n",
      " [    0    12     0     1     0     0     0]\n",
      " [    0     0    39     0     0     0     0]\n",
      " [    0     0     0  2155     0     0     0]\n",
      " [    0     0     0     0   809     0     0]\n",
      " [    0     0     0     1     0     3     0]\n",
      " [    0     0     1     0     0     0     1]]\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=-1, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)Accuracy: 1.00 (+/- 0.00)\n",
      "Num Features: 9\n",
      "Selected Features: [ True  True  True  True  True  True  True  True  True]\n",
      "Feature Ranking: [1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "model(rf,X1_train,y1_train,X1_test,y1_test)\n",
    "crossval(rf,df_row_reindex_X,df_row_reindex_y)\n",
    "rfe(rf,df_row_reindex_X,df_row_reindex_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Let us evaluate the Logistic Regression algorithm on our second data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)Accuracy: 93.10344827586206\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.99      0.96     11478\n",
      "           2       0.00      0.00      0.00        13\n",
      "           3       0.00      0.00      0.00        39\n",
      "           4       0.91      0.61      0.73      2155\n",
      "           5       1.00      1.00      1.00       809\n",
      "           6       0.00      0.00      0.00         4\n",
      "           7       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.93     14500\n",
      "   macro avg       0.41      0.37      0.38     14500\n",
      "weighted avg       0.93      0.93      0.92     14500\n",
      "\n",
      "[[11372     0     0   104     0     0     2]\n",
      " [    8     0     0     5     0     0     0]\n",
      " [   17     0     0    22     0     0     0]\n",
      " [  835     0     0  1320     0     0     0]\n",
      " [    1     0     0     0   808     0     0]\n",
      " [    0     0     0     4     0     0     0]\n",
      " [    2     0     0     0     0     0     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaygun/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/kaygun/.local/lib/python3.8/site-packages/sklearn/svm/_base.py:946: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)Accuracy: 0.93 (+/- 0.01)\n",
      "Num Features: 9\n",
      "Selected Features: [ True  True  True  True  True  True  True  True  True]\n",
      "Feature Ranking: [1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "model(logreg, X1_train,y1_train,X1_test,y1_test)\n",
    "crossval(logreg,df_row_reindex_X,df_row_reindex_y)\n",
    "rfe(logreg,df_row_reindex_X,df_row_reindex_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "Let us evaluate the Support Vector Machines algorithm on our second data set. (My machine was unable to perform this operation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(svm,X1_train,y1_train,X1_test,y1_test)\n",
    "crossval(svm,df_row_reindex_X,df_row_reindex_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "Let us evaluate the XGBoost algorithm on our second data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)Accuracy: 99.99310344827586\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00     11478\n",
      "           2       1.00      0.92      0.96        13\n",
      "           3       1.00      1.00      1.00        39\n",
      "           4       1.00      1.00      1.00      2155\n",
      "           5       1.00      1.00      1.00       809\n",
      "           6       1.00      1.00      1.00         4\n",
      "           7       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00     14500\n",
      "   macro avg       1.00      0.99      0.99     14500\n",
      "weighted avg       1.00      1.00      1.00     14500\n",
      "\n",
      "[[11478     0     0     0     0     0     0]\n",
      " [    0    12     0     1     0     0     0]\n",
      " [    0     0    39     0     0     0     0]\n",
      " [    0     0     0  2155     0     0     0]\n",
      " [    0     0     0     0   809     0     0]\n",
      " [    0     0     0     0     0     4     0]\n",
      " [    0     0     0     0     0     0     2]]\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)Accuracy: 1.00 (+/- 0.00)\n",
      "Num Features: 9\n",
      "Selected Features: [ True  True  True  True  True  True  True  True  True]\n",
      "Feature Ranking: [1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "model(xgb,X1_train,y1_train,X1_test,y1_test)\n",
    "crossval(xgb,df_row_reindex_X,df_row_reindex_y)\n",
    "rfe(xgb,df_row_reindex_X,df_row_reindex_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes\n",
    "\n",
    "Let us evaluate the Gaussian Naive Bayes algorithm on our second data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None, var_smoothing=1e-09)Accuracy: 82.6551724137931\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.88      0.92     11478\n",
      "           2       0.01      0.92      0.02        13\n",
      "           3       0.11      0.59      0.19        39\n",
      "           4       0.89      0.54      0.67      2155\n",
      "           5       0.99      0.82      0.90       809\n",
      "           6       0.40      1.00      0.57         4\n",
      "           7       0.00      1.00      0.01         2\n",
      "\n",
      "    accuracy                           0.83     14500\n",
      "   macro avg       0.48      0.82      0.47     14500\n",
      "weighted avg       0.94      0.83      0.88     14500\n",
      "\n",
      "[[10116   463   185   143     4     5   562]\n",
      " [    1    12     0     0     0     0     0]\n",
      " [    7     1    23     0     1     0     7]\n",
      " [  502   491     0  1162     0     0     0]\n",
      " [    0   142     0     0   666     1     0]\n",
      " [    0     0     0     0     0     4     0]\n",
      " [    0     0     0     0     0     0     2]]\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)Accuracy: 0.81 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "model(gnb,X1_train,y1_train,X1_test,y1_test)\n",
    "crossval(gnb,df_row_reindex_X,df_row_reindex_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments on Our Third Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ATES</th>\n",
       "      <th>BULANTI</th>\n",
       "      <th>BEL-AGRI</th>\n",
       "      <th>SUREKLI-WC</th>\n",
       "      <th>IDRAR-SIRASINDA-AGRI</th>\n",
       "      <th>URETRADA-YANMA-SISME-KASINTI</th>\n",
       "      <th>MESANE-ILTIHABI</th>\n",
       "      <th>BOBREK-ILTIHABI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>355</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>359</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>359</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>360</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>360</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ATES  BULANTI  BEL-AGRI  SUREKLI-WC  IDRAR-SIRASINDA-AGRI  \\\n",
       "0   355        0         1           0                     0   \n",
       "1   359        0         0           1                     1   \n",
       "2   359        0         1           0                     0   \n",
       "3   360        0         0           1                     1   \n",
       "4   360        0         1           0                     0   \n",
       "\n",
       "   URETRADA-YANMA-SISME-KASINTI  MESANE-ILTIHABI  BOBREK-ILTIHABI  \n",
       "0                             0                0                0  \n",
       "1                             1                1                0  \n",
       "2                             0                0                0  \n",
       "3                             1                1                0  \n",
       "4                             0                0                0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = pd.read_csv(\"data-hastalik.csv\",sep = ';' )\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection and splitting the train and test data sets\n",
    "\n",
    "Let us denote the features we are going to use with X and response variable with y. With the drop function, we get the columns other than the columns 'MESANE-ILTIHABI' and 'BOBREK-ILTIHABI'. For the response variable, we just take the column named 'MESANE-ILTIHABI'. Then we determine the size of our training and test sets. For this dataset, 80% of all data is used for training and the remaining 20% is used for testing using the function \"train_test_split\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = db.drop([\"MESANE-ILTIHABI\",\"BOBREK-ILTIHABI\"],axis=1)\n",
    "y = db['MESANE-ILTIHABI']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = db.drop([\"MESANE-ILTIHABI\",\"BOBREK-ILTIHABI\"],axis=1)\n",
    "b = db['BOBREK-ILTIHABI']\n",
    "A_train, A_test, b_train, b_test = train_test_split(A, b,test_size=0.2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create array for loop\n",
    "\n",
    "We create two arrays for our third dataset. We will use these arrays for the functions we will write below. The difference between the two arrays is that the rfe function does not work in SVM and GNB algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [dt, rf, logreg, svm, xgb, gnb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Models\n",
    "\n",
    "Let us evaluate the all model on our third data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')Accuracy: 100.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        24\n",
      "   macro avg       1.00      1.00      1.00        24\n",
      "weighted avg       1.00      1.00      1.00        24\n",
      "\n",
      "[[14  0]\n",
      " [ 0 10]]\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')Accuracy: 100.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        24\n",
      "   macro avg       1.00      1.00      1.00        24\n",
      "weighted avg       1.00      1.00      1.00        24\n",
      "\n",
      "[[15  0]\n",
      " [ 0  9]]\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')Accuracy: 0.97 (+/- 0.13)\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')Accuracy: 1.00 (+/- 0.00)\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')Accuracy: 0.86 (+/- 0.36)\n",
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')Accuracy: 0.97 (+/- 0.20)\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=-1, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)Accuracy: 100.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        24\n",
      "   macro avg       1.00      1.00      1.00        24\n",
      "weighted avg       1.00      1.00      1.00        24\n",
      "\n",
      "[[14  0]\n",
      " [ 0 10]]\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=-1, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)Accuracy: 100.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        24\n",
      "   macro avg       1.00      1.00      1.00        24\n",
      "weighted avg       1.00      1.00      1.00        24\n",
      "\n",
      "[[15  0]\n",
      " [ 0  9]]\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=-1, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)Accuracy: 1.00 (+/- 0.00)\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=-1, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)Accuracy: 1.00 (+/- 0.00)\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=-1, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)Accuracy: 1.00 (+/- 0.00)\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=-1, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)Accuracy: 0.97 (+/- 0.20)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)Accuracy: 100.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        24\n",
      "   macro avg       1.00      1.00      1.00        24\n",
      "weighted avg       1.00      1.00      1.00        24\n",
      "\n",
      "[[14  0]\n",
      " [ 0 10]]\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)Accuracy: 100.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        24\n",
      "   macro avg       1.00      1.00      1.00        24\n",
      "weighted avg       1.00      1.00      1.00        24\n",
      "\n",
      "[[15  0]\n",
      " [ 0  9]]\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)Accuracy: 1.00 (+/- 0.00)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)Accuracy: 1.00 (+/- 0.00)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)Accuracy: 1.00 (+/- 0.00)\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)Accuracy: 1.00 (+/- 0.00)\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)Accuracy: 87.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.88        14\n",
      "           1       0.77      1.00      0.87        10\n",
      "\n",
      "    accuracy                           0.88        24\n",
      "   macro avg       0.88      0.89      0.87        24\n",
      "weighted avg       0.90      0.88      0.88        24\n",
      "\n",
      "[[11  3]\n",
      " [ 0 10]]\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)Accuracy: 91.66666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93        15\n",
      "           1       0.82      1.00      0.90         9\n",
      "\n",
      "    accuracy                           0.92        24\n",
      "   macro avg       0.91      0.93      0.91        24\n",
      "weighted avg       0.93      0.92      0.92        24\n",
      "\n",
      "[[13  2]\n",
      " [ 0  9]]\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)Accuracy: 0.54 (+/- 0.17)\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)Accuracy: 0.63 (+/- 0.33)\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)Accuracy: 0.74 (+/- 0.33)\n",
      "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)Accuracy: 0.84 (+/- 0.39)\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)Accuracy: 100.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        24\n",
      "   macro avg       1.00      1.00      1.00        24\n",
      "weighted avg       1.00      1.00      1.00        24\n",
      "\n",
      "[[14  0]\n",
      " [ 0 10]]\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)Accuracy: 100.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00        24\n",
      "   macro avg       1.00      1.00      1.00        24\n",
      "weighted avg       1.00      1.00      1.00        24\n",
      "\n",
      "[[15  0]\n",
      " [ 0  9]]\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)Accuracy: 0.97 (+/- 0.10)\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)Accuracy: 0.97 (+/- 0.15)\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)Accuracy: 0.86 (+/- 0.36)\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)Accuracy: 0.97 (+/- 0.15)\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)Accuracy: 70.83333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67        14\n",
      "           1       0.59      1.00      0.74        10\n",
      "\n",
      "    accuracy                           0.71        24\n",
      "   macro avg       0.79      0.75      0.70        24\n",
      "weighted avg       0.83      0.71      0.70        24\n",
      "\n",
      "[[ 7  7]\n",
      " [ 0 10]]\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)Accuracy: 91.66666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94        15\n",
      "           1       1.00      0.78      0.88         9\n",
      "\n",
      "    accuracy                           0.92        24\n",
      "   macro avg       0.94      0.89      0.91        24\n",
      "weighted avg       0.93      0.92      0.91        24\n",
      "\n",
      "[[15  0]\n",
      " [ 2  7]]\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)Accuracy: 0.82 (+/- 0.19)\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)Accuracy: 0.82 (+/- 0.32)\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)Accuracy: 0.92 (+/- 0.33)\n",
      "GaussianNB(priors=None, var_smoothing=1e-09)Accuracy: 0.95 (+/- 0.25)\n"
     ]
    }
   ],
   "source": [
    "for i in models:\n",
    "    model(i, X_train, X_test, y_train, y_test)\n",
    "    model(i, A_train, A_test, b_train, b_test)\n",
    "    crossval(i, X, y, 5)\n",
    "    crossval(i, X, y, 10)\n",
    "    crossval(i, A, b, 5)\n",
    "    crossval(i, A, b, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination\n",
    "\n",
    "Evaluate rfe on our third data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
      "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
      "                       random_state=None, splitter='best')\n",
      "Num Features: 2\n",
      "Selected Features: [False False False  True  True False]\n",
      "Feature Ranking: [2 5 4 1 1 3]\n",
      "Num Features: 2\n",
      "Selected Features: [ True False  True False False False]\n",
      "Feature Ranking: [1 5 1 4 3 2]\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=-1, oob_score=False, random_state=0, verbose=0,\n",
      "                       warm_start=False)\n",
      "Num Features: 2\n",
      "Selected Features: [False False False  True  True False]\n",
      "Feature Ranking: [3 5 2 1 1 4]\n",
      "Num Features: 2\n",
      "Selected Features: [ True False  True False False False]\n",
      "Feature Ranking: [1 2 1 5 4 3]\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "Num Features: 2\n",
      "Selected Features: [False False False  True  True False]\n",
      "Feature Ranking: [5 3 2 1 1 4]\n",
      "Num Features: 2\n",
      "Selected Features: [False  True  True False False False]\n",
      "Feature Ranking: [5 1 1 3 4 2]\n",
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
      "              learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
      "              nthread=None, objective='binary:logistic', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "              silent=None, subsample=1, verbosity=1)\n",
      "Num Features: 2\n",
      "Selected Features: [False False False  True  True False]\n",
      "Feature Ranking: [2 3 4 1 1 5]\n",
      "Num Features: 2\n",
      "Selected Features: [ True False  True False False False]\n",
      "Feature Ranking: [1 5 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "model = [dt, rf, logreg, xgb]\n",
    "for i in model:\n",
    "    print(i)\n",
    "    rfe(i, X, y)\n",
    "    rfe(i, A, b)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
