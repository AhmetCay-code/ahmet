{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import graphviz\n",
    "import pydotplus\n",
    "import io\n",
    "import imageio\n",
    "from scipy import misc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MUSK</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>MUSK</td>\n",
       "      <td>46</td>\n",
       "      <td>-108</td>\n",
       "      <td>-60</td>\n",
       "      <td>-69</td>\n",
       "      <td>-117</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>-161</td>\n",
       "      <td>-8</td>\n",
       "      <td>...</td>\n",
       "      <td>-308</td>\n",
       "      <td>52</td>\n",
       "      <td>-7</td>\n",
       "      <td>39</td>\n",
       "      <td>126</td>\n",
       "      <td>156</td>\n",
       "      <td>-50</td>\n",
       "      <td>-112</td>\n",
       "      <td>96</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>MUSK</td>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-6</td>\n",
       "      <td>57</td>\n",
       "      <td>-171</td>\n",
       "      <td>-39</td>\n",
       "      <td>...</td>\n",
       "      <td>-59</td>\n",
       "      <td>-2</td>\n",
       "      <td>52</td>\n",
       "      <td>103</td>\n",
       "      <td>136</td>\n",
       "      <td>169</td>\n",
       "      <td>-61</td>\n",
       "      <td>-136</td>\n",
       "      <td>79</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>MUSK</td>\n",
       "      <td>46</td>\n",
       "      <td>-194</td>\n",
       "      <td>-145</td>\n",
       "      <td>28</td>\n",
       "      <td>-117</td>\n",
       "      <td>73</td>\n",
       "      <td>57</td>\n",
       "      <td>-168</td>\n",
       "      <td>-39</td>\n",
       "      <td>...</td>\n",
       "      <td>-134</td>\n",
       "      <td>-154</td>\n",
       "      <td>57</td>\n",
       "      <td>143</td>\n",
       "      <td>142</td>\n",
       "      <td>165</td>\n",
       "      <td>-67</td>\n",
       "      <td>-145</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>MUSK</td>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>-170</td>\n",
       "      <td>-39</td>\n",
       "      <td>...</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>136</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>MUSK</td>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>-170</td>\n",
       "      <td>-39</td>\n",
       "      <td>...</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>137</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MUSK   2    3    4   5    6   7   8    9  10  ...  159  160  161  162  163  \\\n",
       "0  MUSK  46 -108  -60 -69 -117  49  38 -161  -8  ... -308   52   -7   39  126   \n",
       "1  MUSK  41 -188 -145  22 -117  -6  57 -171 -39  ...  -59   -2   52  103  136   \n",
       "2  MUSK  46 -194 -145  28 -117  73  57 -168 -39  ... -134 -154   57  143  142   \n",
       "3  MUSK  41 -188 -145  22 -117  -7  57 -170 -39  ...  -60   -4   52  104  136   \n",
       "4  MUSK  41 -188 -145  22 -117  -7  57 -170 -39  ...  -60   -4   52  104  137   \n",
       "\n",
       "   164  165  166  167  168  \n",
       "0  156  -50 -112   96  1.0  \n",
       "1  169  -61 -136   79  1.0  \n",
       "2  165  -67 -145   39  1.0  \n",
       "3  168  -60 -135   80  1.0  \n",
       "4  168  -60 -135   80  1.0  \n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database = pd.read_csv(\"data.csv\", sep=';')\n",
    "database.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = database.drop([\"MUSK\",\"168\"],axis=1)\n",
    "y = database['168']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.66666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98       554\n",
      "         1.0       0.90      0.90      0.90       106\n",
      "\n",
      "    accuracy                           0.97       660\n",
      "   macro avg       0.94      0.94      0.94       660\n",
      "weighted avg       0.97      0.97      0.97       660\n",
      "\n",
      "[[543  11]\n",
      " [ 11  95]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.009234369055268522, 0.0, 0.0021594025126719443, 0.0024852549724232336, 0.0, 0.0022520372087311974, 0.0, 0.0022595600224035745, 0.06397114923145306, 0.0012572907422125012, 0.00213935877854596, 0.0, 0.00125606291922206, 0.0, 0.0, 0.0011525165136947934, 0.011325283063724729, 0.0, 0.001662808364824133, 0.008088888141911313, 0.005479692151476155, 0.01408731718694931, 0.001621690008800933, 0.0, 0.003121759955327445, 0.0, 0.0, 0.0, 0.0026967439336151675, 0.0, 0.0, 0.02403929825441351, 0.002307151623922115, 0.003218180726701615, 0.0012625611738286727, 0.15457710430572807, 0.001669763708663105, 0.005496745190755983, 0.002532130984630203, 0.000972435808429982, 0.000972435808429982, 0.006043081396384767, 0.010427592026875517, 0.0012299087296779264, 0.0, 0.0004112810946047955, 0.0, 0.0024343678550266825, 0.0036268002179206792, 0.030456131221018385, 0.0022399269262553855, 0.0, 0.0, 0.009748368845001796, 0.016353314909560285, 0.0024782303389010336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.013589817619512536, 0.006217250719402908, 0.0, 0.0008643873852710952, 0.009539844623213107, 0.0012155447605374777, 0.001279520800565766, 0.001458653712644973, 0.0006482905389533214, 0.0, 0.0, 0.0011787100708242208, 0.0, 0.02559792801828406, 0.0, 0.0, 0.0, 0.0, 0.03620155702635368, 0.0012673533349739795, 0.0, 0.02509076323901545, 0.0012734278443725963, 0.0, 0.0, 0.0, 0.005147765854734573, 0.01027019282830444, 0.001246712574910234, 0.0, 0.000419920686231873, 0.008072573498947763, 0.00017375169944134275, 0.011179189313551198, 0.0022866493283981355, 0.0, 0.0, 0.010388822026459807, 0.0006482905389533214, 0.0, 0.0, 0.0011113552096342655, 0.0, 0.0, 0.00236709160889623, 0.001944871616859964, 0.0, 0.0008643873852710952, 0.0, 0.00034192830247094827, 0.0, 0.002122833725592248, 0.001255935737361613, 0.0011154410743755684, 0.005892124495164462, 0.002160968463177738, 0.0, 0.0, 0.001961078880333796, 0.005011364466979949, 0.02390355377528099, 0.0, 0.10276344593915873, 0.002529136152004663, 0.0270101285970873, 0.002116095893996996, 0.005053311446932161, 0.0, 0.0022362577683866782, 0.0012462864940575651, 0.047993352345404604, 0.0062562114863287084, 0.0, 0.0, 0.0046331219695113154, 0.0157336807913661, 0.0, 0.0, 0.027521640832747152, 0.0011787100708242208, 0.0, 0.002496864356923667, 0.0033006466873584392, 0.0, 0.04165990856254416, 0.0, 0.0, 0.0, 0.0, 0.014521009157929076, 0.0, 0.022892825507855874, 0.0, 0.0022318792846041636, 0.001944871616859964, 0.0, 0.005875929983274795, 0.0006449052097681918, 0.0, 0.0, 0.004188954251698384, 0.0, 0.007555556756485477, 0.0, 0.0023574201416484415]\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "score= accuracy_score(y_test, y_pred) * 100\n",
    "print(score)\n",
    "report_dt=classification_report(y_test, y_pred)\n",
    "print(report_dt)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "pyplot.bar(range(len(dt.feature_importances_)), dt.feature_importances_)\n",
    "pyplot.show()\n",
    "print(list(dt.feature_importances_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.75757575757575\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.98      0.97       554\n",
      "         1.0       0.89      0.84      0.86       106\n",
      "\n",
      "    accuracy                           0.96       660\n",
      "   macro avg       0.93      0.91      0.92       660\n",
      "weighted avg       0.96      0.96      0.96       660\n",
      "\n",
      "[[543  11]\n",
      " [ 17  89]]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.109863\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                    168   No. Observations:                 6599\n",
      "Model:                          Logit   Df Residuals:                     6433\n",
      "Method:                           MLE   Df Model:                          165\n",
      "Date:                Sun, 01 Mar 2020   Pseudo R-squ.:                  0.7444\n",
      "Time:                        23:46:28   Log-Likelihood:                -724.99\n",
      "converged:                       True   LL-Null:                       -2836.1\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "2             -0.0968      0.022     -4.440      0.000      -0.140      -0.054\n",
      "3              0.0188      0.006      3.189      0.001       0.007       0.030\n",
      "4             -0.1008      0.022     -4.481      0.000      -0.145      -0.057\n",
      "5             -0.0202      0.007     -3.006      0.003      -0.033      -0.007\n",
      "6              0.0012      0.002      0.546      0.585      -0.003       0.005\n",
      "7             -0.0143      0.005     -3.038      0.002      -0.024      -0.005\n",
      "8             -0.0019      0.020     -0.097      0.923      -0.041       0.037\n",
      "9              0.0103      0.004      2.780      0.005       0.003       0.018\n",
      "10            -0.0431      0.011     -3.910      0.000      -0.065      -0.021\n",
      "11            -0.0367      0.008     -4.408      0.000      -0.053      -0.020\n",
      "12             0.0097      0.007      1.328      0.184      -0.005       0.024\n",
      "13            -0.0416      0.018     -2.328      0.020      -0.077      -0.007\n",
      "14            -0.0067      0.009     -0.770      0.441      -0.024       0.010\n",
      "15            -0.0404      0.016     -2.468      0.014      -0.072      -0.008\n",
      "16            -0.0220      0.010     -2.121      0.034      -0.042      -0.002\n",
      "17            -0.0074      0.003     -2.551      0.011      -0.013      -0.002\n",
      "18             0.0379      0.005      7.131      0.000       0.027       0.048\n",
      "19            -0.0039      0.005     -0.821      0.412      -0.013       0.005\n",
      "20            -0.0016      0.010     -0.151      0.880      -0.022       0.019\n",
      "21            -0.0015      0.005     -0.298      0.766      -0.011       0.008\n",
      "22             0.0023      0.009      0.254      0.800      -0.016       0.020\n",
      "23            -0.0433      0.025     -1.747      0.081      -0.092       0.005\n",
      "24             0.0064      0.013      0.505      0.614      -0.018       0.031\n",
      "25            -0.0018      0.002     -0.932      0.351      -0.006       0.002\n",
      "26            -0.0203      0.007     -2.770      0.006      -0.035      -0.006\n",
      "27            -0.0290      0.007     -3.971      0.000      -0.043      -0.015\n",
      "28             0.0198      0.012      1.649      0.099      -0.004       0.043\n",
      "29            -0.0081      0.004     -2.206      0.027      -0.015      -0.001\n",
      "30            -0.0338      0.011     -3.048      0.002      -0.056      -0.012\n",
      "31            -0.0122      0.011     -1.062      0.288      -0.035       0.010\n",
      "32            -0.0101      0.005     -1.860      0.063      -0.021       0.001\n",
      "33            -0.0215      0.007     -3.098      0.002      -0.035      -0.008\n",
      "34            -0.0619      0.013     -4.637      0.000      -0.088      -0.036\n",
      "35            -0.0048      0.004     -1.083      0.279      -0.013       0.004\n",
      "36            -0.0449      0.012     -3.704      0.000      -0.069      -0.021\n",
      "37            -0.0295      0.011     -2.617      0.009      -0.052      -0.007\n",
      "38            -0.0083      0.003     -2.715      0.007      -0.014      -0.002\n",
      "39             0.0247      0.013      1.944      0.052      -0.000       0.050\n",
      "40            -0.0082      0.008     -1.049      0.294      -0.023       0.007\n",
      "41            -0.0205      0.025     -0.825      0.409      -0.069       0.028\n",
      "42             0.0462      0.022      2.100      0.036       0.003       0.089\n",
      "43            -0.0226      0.006     -3.770      0.000      -0.034      -0.011\n",
      "44             0.0326      0.010      3.130      0.002       0.012       0.053\n",
      "45             0.0011      0.013      0.084      0.933      -0.025       0.027\n",
      "46            -0.0278      0.020     -1.357      0.175      -0.068       0.012\n",
      "47             0.0007      0.004      0.189      0.850      -0.007       0.008\n",
      "48            -0.0131      0.004     -3.628      0.000      -0.020      -0.006\n",
      "49             0.0114      0.008      1.469      0.142      -0.004       0.027\n",
      "50            -0.0114      0.006     -1.836      0.066      -0.024       0.001\n",
      "51             0.0257      0.007      3.472      0.001       0.011       0.040\n",
      "52             0.0343      0.012      2.933      0.003       0.011       0.057\n",
      "53             0.0680      0.018      3.881      0.000       0.034       0.102\n",
      "54             0.0424      0.016      2.616      0.009       0.011       0.074\n",
      "55            -0.0002      0.002     -0.085      0.933      -0.004       0.004\n",
      "56            -0.0002      0.005     -0.048      0.961      -0.010       0.009\n",
      "57             0.0095      0.004      2.225      0.026       0.001       0.018\n",
      "58            -0.0460      0.019     -2.372      0.018      -0.084      -0.008\n",
      "59            -0.0209      0.005     -4.369      0.000      -0.030      -0.012\n",
      "60             0.0379      0.010      3.810      0.000       0.018       0.057\n",
      "61            -0.0100      0.010     -1.026      0.305      -0.029       0.009\n",
      "62            -0.0085      0.004     -2.062      0.039      -0.017      -0.000\n",
      "63            -0.0041      0.005     -0.752      0.452      -0.015       0.007\n",
      "64             0.0107      0.015      0.729      0.466      -0.018       0.039\n",
      "65             0.0032      0.006      0.538      0.590      -0.009       0.015\n",
      "66            -0.0141      0.010     -1.381      0.167      -0.034       0.006\n",
      "67             0.0804      0.021      3.775      0.000       0.039       0.122\n",
      "68            -0.0148      0.007     -2.054      0.040      -0.029      -0.001\n",
      "69             0.0080      0.021      0.386      0.699      -0.032       0.048\n",
      "70            -0.0021      0.003     -0.611      0.541      -0.009       0.005\n",
      "71             0.0185      0.026      0.722      0.470      -0.032       0.069\n",
      "72             0.0658      0.028      2.359      0.018       0.011       0.120\n",
      "73            -0.0416      0.012     -3.534      0.000      -0.065      -0.019\n",
      "74            -0.0180      0.008     -2.389      0.017      -0.033      -0.003\n",
      "75             0.0227      0.018      1.286      0.198      -0.012       0.057\n",
      "76            -0.0172      0.008     -2.195      0.028      -0.032      -0.002\n",
      "77             0.0225      0.010      2.285      0.022       0.003       0.042\n",
      "78            -0.0048      0.008     -0.612      0.540      -0.020       0.011\n",
      "79             0.0254      0.008      3.129      0.002       0.009       0.041\n",
      "80             0.0206      0.017      1.237      0.216      -0.012       0.053\n",
      "81             0.0020      0.005      0.430      0.667      -0.007       0.011\n",
      "82            -0.0265      0.011     -2.518      0.012      -0.047      -0.006\n",
      "83            -0.0044      0.027     -0.164      0.870      -0.057       0.048\n",
      "84            -0.0230      0.010     -2.389      0.017      -0.042      -0.004\n",
      "85             0.0057      0.003      1.859      0.063      -0.000       0.012\n",
      "86             0.0339      0.008      4.126      0.000       0.018       0.050\n",
      "87            -0.0253      0.012     -2.128      0.033      -0.049      -0.002\n",
      "88             0.0328      0.009      3.672      0.000       0.015       0.050\n",
      "89            -0.0090      0.003     -3.077      0.002      -0.015      -0.003\n",
      "90             0.0100      0.009      1.117      0.264      -0.008       0.028\n",
      "91             0.0083      0.007      1.161      0.246      -0.006       0.022\n",
      "92            -0.0015      0.003     -0.478      0.633      -0.007       0.005\n",
      "93             0.0153      0.012      1.304      0.192      -0.008       0.038\n",
      "94             0.0329      0.014      2.303      0.021       0.005       0.061\n",
      "95             0.0160      0.004      3.663      0.000       0.007       0.025\n",
      "96             0.0001      0.012      0.010      0.992      -0.024       0.024\n",
      "97            -0.0165      0.004     -4.419      0.000      -0.024      -0.009\n",
      "98             0.0028      0.004      0.785      0.433      -0.004       0.010\n",
      "99            -0.0498      0.011     -4.666      0.000      -0.071      -0.029\n",
      "100            0.1060      0.016      6.645      0.000       0.075       0.137\n",
      "101           -0.0161      0.022     -0.733      0.463      -0.059       0.027\n",
      "102           -0.0517      0.013     -4.003      0.000      -0.077      -0.026\n",
      "103           -0.0148      0.012     -1.231      0.218      -0.038       0.009\n",
      "104            0.0269      0.010      2.735      0.006       0.008       0.046\n",
      "105           -0.0185      0.013     -1.397      0.163      -0.045       0.007\n",
      "106            0.0284      0.017      1.701      0.089      -0.004       0.061\n",
      "107           -0.0032      0.004     -0.839      0.402      -0.011       0.004\n",
      "108           -0.0139      0.003     -4.639      0.000      -0.020      -0.008\n",
      "109            0.0117      0.005      2.547      0.011       0.003       0.021\n",
      "110            0.0083      0.007      1.205      0.228      -0.005       0.022\n",
      "111            0.0079      0.015      0.527      0.598      -0.021       0.037\n",
      "112           -0.0101      0.008     -1.332      0.183      -0.025       0.005\n",
      "113           -0.0057      0.012     -0.456      0.649      -0.030       0.019\n",
      "114            0.0246      0.016      1.573      0.116      -0.006       0.055\n",
      "115            0.0061      0.019      0.314      0.754      -0.032       0.044\n",
      "116           -0.0166      0.005     -3.227      0.001      -0.027      -0.007\n",
      "117            0.0104      0.003      3.549      0.000       0.005       0.016\n",
      "118            0.0091      0.005      1.931      0.053      -0.000       0.018\n",
      "119            0.0269      0.006      4.414      0.000       0.015       0.039\n",
      "120            0.0767      0.014      5.650      0.000       0.050       0.103\n",
      "121           -0.0218      0.007     -3.210      0.001      -0.035      -0.008\n",
      "122            0.0206      0.010      2.050      0.040       0.001       0.040\n",
      "123            0.0191      0.006      2.949      0.003       0.006       0.032\n",
      "124           -0.0262      0.008     -3.176      0.001      -0.042      -0.010\n",
      "125           -0.0086      0.006     -1.494      0.135      -0.020       0.003\n",
      "126           -0.0213      0.005     -4.031      0.000      -0.032      -0.011\n",
      "127            0.0381      0.011      3.607      0.000       0.017       0.059\n",
      "128           -0.0192      0.009     -2.200      0.028      -0.036      -0.002\n",
      "129            0.0139      0.010      1.444      0.149      -0.005       0.033\n",
      "130            0.0143      0.007      2.113      0.035       0.001       0.028\n",
      "131            0.0250      0.008      3.210      0.001       0.010       0.040\n",
      "132           -0.0143      0.009     -1.595      0.111      -0.032       0.003\n",
      "133           -0.0041      0.004     -1.131      0.258      -0.011       0.003\n",
      "134           -0.0111      0.008     -1.409      0.159      -0.026       0.004\n",
      "135            0.0224      0.012      1.886      0.059      -0.001       0.046\n",
      "136           -0.0048      0.010     -0.472      0.637      -0.025       0.015\n",
      "137           -0.0210      0.011     -1.944      0.052      -0.042       0.000\n",
      "138           -0.0108      0.007     -1.510      0.131      -0.025       0.003\n",
      "139           -0.0180      0.005     -3.665      0.000      -0.028      -0.008\n",
      "140           -0.0283      0.010     -2.761      0.006      -0.048      -0.008\n",
      "141            0.0166      0.007      2.452      0.014       0.003       0.030\n",
      "142            0.0065      0.008      0.779      0.436      -0.010       0.023\n",
      "143        -1.961e-05      0.020     -0.001      0.999      -0.039       0.039\n",
      "144           -0.0216      0.020     -1.101      0.271      -0.060       0.017\n",
      "145            0.0623      0.015      4.025      0.000       0.032       0.093\n",
      "146           -0.0014      0.005     -0.271      0.786      -0.011       0.009\n",
      "147            0.0055      0.005      1.159      0.246      -0.004       0.015\n",
      "148           -0.0128      0.007     -1.753      0.080      -0.027       0.002\n",
      "149            0.0083      0.021      0.387      0.699      -0.034       0.050\n",
      "150           -0.0522      0.015     -3.584      0.000      -0.081      -0.024\n",
      "151            0.0168      0.019      0.885      0.376      -0.020       0.054\n",
      "152            0.0452      0.017      2.689      0.007       0.012       0.078\n",
      "153            0.0132      0.014      0.922      0.357      -0.015       0.041\n",
      "154           -0.0050      0.021     -0.236      0.813      -0.046       0.036\n",
      "155           -0.0106      0.012     -0.916      0.360      -0.033       0.012\n",
      "156            0.0231      0.008      3.029      0.002       0.008       0.038\n",
      "157            0.0263      0.009      3.044      0.002       0.009       0.043\n",
      "158           -0.0019      0.002     -0.978      0.328      -0.006       0.002\n",
      "159           -0.0050      0.002     -2.493      0.013      -0.009      -0.001\n",
      "160            0.0033      0.003      1.107      0.268      -0.003       0.009\n",
      "161            0.0049      0.006      0.759      0.448      -0.008       0.017\n",
      "162            0.0032      0.004      0.764      0.445      -0.005       0.012\n",
      "163           -0.0214      0.010     -2.211      0.027      -0.040      -0.002\n",
      "164           -0.0405      0.009     -4.335      0.000      -0.059      -0.022\n",
      "165           -0.0057      0.008     -0.708      0.479      -0.021       0.010\n",
      "166           -0.0188      0.009     -2.040      0.041      -0.037      -0.001\n",
      "167            0.0068      0.004      1.873      0.061      -0.000       0.014\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.21 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "logreg=LogisticRegression(solver='liblinear',multi_class='ovr')\n",
    "logreg.fit(X_train,y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "score= accuracy_score(y_test, y_pred) * 100\n",
    "print(score)\n",
    "report_logreg=classification_report(y_test, y_pred)\n",
    "print(report_logreg)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.48484848484848\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99       554\n",
      "         1.0       0.98      0.92      0.95       106\n",
      "\n",
      "    accuracy                           0.98       660\n",
      "   macro avg       0.98      0.96      0.97       660\n",
      "weighted avg       0.98      0.98      0.98       660\n",
      "\n",
      "[[552   2]\n",
      " [  8  98]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs = -1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "score= accuracy_score(y_test, y_pred) * 100\n",
    "print(score)\n",
    "report_rf=classification_report(y_test, y_pred)\n",
    "print(report_rf)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "pyplot.bar(range(len(rf.feature_importances_)), rf.feature_importances_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      1.00      0.94       554\n",
      "         1.0       1.00      0.38      0.55       106\n",
      "\n",
      "    accuracy                           0.90       660\n",
      "   macro avg       0.95      0.69      0.75       660\n",
      "weighted avg       0.91      0.90      0.88       660\n",
      "\n",
      "[[554   0]\n",
      " [ 66  40]]\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(gamma='auto')\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "score= accuracy_score(y_test, y_pred) * 100\n",
    "print(score)\n",
    "report_svm=classification_report(y_test, y_pred)\n",
    "print(report_svm)\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS VALIDATIONS / 10 - FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "print(\"DECISION_TREE:\" ,cross_val_score(DecisionTreeClassifier(), X, y, cv=10, scoring =\"accuracy\").mean())\n",
    "print(\"LOGISTIC_REGRESSION:\" ,cross_val_score(LogisticRegression(solver='liblinear',multi_class='ovr'), X, y,cv=10, scoring = \"accuracy\").mean())\n",
    "print(\"SUPPORT_VECTOR_MACHINES:\" ,cross_val_score(SVC(gamma='auto'), X, y, cv=10, scoring =\"accuracy\").mean())\n",
    "print(\"RANDOM_FOREST:\" ,cross_val_score(RandomForestClassifier(n_estimators=40), X, y, cv=10, scoring =\"accuracy\").mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
