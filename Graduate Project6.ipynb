{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import AnovaRM\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "import io\n",
    "import imageio\n",
    "from scipy import misc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MUSK</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MUSK</td>\n",
       "      <td>46</td>\n",
       "      <td>-108</td>\n",
       "      <td>-60</td>\n",
       "      <td>-69</td>\n",
       "      <td>-117</td>\n",
       "      <td>49</td>\n",
       "      <td>38</td>\n",
       "      <td>-161</td>\n",
       "      <td>-8</td>\n",
       "      <td>...</td>\n",
       "      <td>-308</td>\n",
       "      <td>52</td>\n",
       "      <td>-7</td>\n",
       "      <td>39</td>\n",
       "      <td>126</td>\n",
       "      <td>156</td>\n",
       "      <td>-50</td>\n",
       "      <td>-112</td>\n",
       "      <td>96</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MUSK</td>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-6</td>\n",
       "      <td>57</td>\n",
       "      <td>-171</td>\n",
       "      <td>-39</td>\n",
       "      <td>...</td>\n",
       "      <td>-59</td>\n",
       "      <td>-2</td>\n",
       "      <td>52</td>\n",
       "      <td>103</td>\n",
       "      <td>136</td>\n",
       "      <td>169</td>\n",
       "      <td>-61</td>\n",
       "      <td>-136</td>\n",
       "      <td>79</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MUSK</td>\n",
       "      <td>46</td>\n",
       "      <td>-194</td>\n",
       "      <td>-145</td>\n",
       "      <td>28</td>\n",
       "      <td>-117</td>\n",
       "      <td>73</td>\n",
       "      <td>57</td>\n",
       "      <td>-168</td>\n",
       "      <td>-39</td>\n",
       "      <td>...</td>\n",
       "      <td>-134</td>\n",
       "      <td>-154</td>\n",
       "      <td>57</td>\n",
       "      <td>143</td>\n",
       "      <td>142</td>\n",
       "      <td>165</td>\n",
       "      <td>-67</td>\n",
       "      <td>-145</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MUSK</td>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>-170</td>\n",
       "      <td>-39</td>\n",
       "      <td>...</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>136</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MUSK</td>\n",
       "      <td>41</td>\n",
       "      <td>-188</td>\n",
       "      <td>-145</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>-7</td>\n",
       "      <td>57</td>\n",
       "      <td>-170</td>\n",
       "      <td>-39</td>\n",
       "      <td>...</td>\n",
       "      <td>-60</td>\n",
       "      <td>-4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "      <td>137</td>\n",
       "      <td>168</td>\n",
       "      <td>-60</td>\n",
       "      <td>-135</td>\n",
       "      <td>80</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MUSK   2    3    4   5    6   7   8    9  10  ...  159  160  161  162  163  \\\n",
       "0  MUSK  46 -108  -60 -69 -117  49  38 -161  -8  ... -308   52   -7   39  126   \n",
       "1  MUSK  41 -188 -145  22 -117  -6  57 -171 -39  ...  -59   -2   52  103  136   \n",
       "2  MUSK  46 -194 -145  28 -117  73  57 -168 -39  ... -134 -154   57  143  142   \n",
       "3  MUSK  41 -188 -145  22 -117  -7  57 -170 -39  ...  -60   -4   52  104  136   \n",
       "4  MUSK  41 -188 -145  22 -117  -7  57 -170 -39  ...  -60   -4   52  104  137   \n",
       "\n",
       "   164  165  166  167  168  \n",
       "0  156  -50 -112   96  1.0  \n",
       "1  169  -61 -136   79  1.0  \n",
       "2  165  -67 -145   39  1.0  \n",
       "3  168  -60 -135   80  1.0  \n",
       "4  168  -60 -135   80  1.0  \n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database = pd.read_csv(\"data.csv\", sep=';')\n",
    "database.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = database.drop([\"MUSK\",\"168\"],axis=1)\n",
    "y = database['168']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.51515151515152\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98       552\n",
      "         1.0       0.89      0.90      0.89       108\n",
      "\n",
      "    accuracy                           0.97       660\n",
      "   macro avg       0.93      0.94      0.94       660\n",
      "weighted avg       0.97      0.97      0.97       660\n",
      "\n",
      "[[540  12]\n",
      " [ 11  97]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0044650277652019685, 0.0005356184083745744, 0.0, 0.0, 0.0037268612358491242, 0.0025274804323046672, 0.0, 0.06824551395722574, 0.00459885110818776, 0.0, 0.0012061373703402723, 0.0, 0.0, 0.0027670210260747406, 0.0028822696158098096, 0.0, 0.001641813163004221, 0.0, 0.007185313101027124, 0.0025510265102415192, 0.010064898173105526, 0.0, 0.0, 0.011439073669861364, 0.0, 0.0, 0.0, 0.00030680995949663173, 0.0022514564246351744, 0.004662184835353743, 0.028846156773825075, 0.005923062286347921, 0.0012305531065819786, 0.0012840826848344428, 0.1558477676424949, 0.0, 0.005493710862458798, 0.003405385638281141, 0.0010391337344470032, 0.0, 0.005896529297421237, 0.008639308952628577, 0.002871474356383424, 0.0, 0.00023948785286083307, 0.0012418365277416738, 0.0, 0.007312016362883858, 0.022696065725650087, 0.0024959584797991773, 0.0, 0.001855595954369649, 0.007600521029098081, 0.015339265179190962, 0.002464109636269723, 0.0011690254512528792, 0.0, 0.0, 0.0009741878760440657, 0.0009741878760440657, 0.01760141487622242, 0.010450538953123152, 0.0, 0.004986804088906729, 0.0, 0.0, 0.0, 0.0017155452317788027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.007198303343249398, 0.002143213327296944, 0.0, 0.0, 0.0, 0.04198436404900514, 0.0, 0.0, 0.02786830286543162, 0.0012469604813364038, 0.01662423450849089, 0.0, 0.0, 0.0021981675151763534, 0.023457944342509928, 0.0, 0.00417509089733171, 0.0017079376627740573, 0.008467015614012622, 0.0002050921844303255, 0.002666198397594287, 0.003421550023492306, 0.0, 0.0, 0.008916230059560486, 0.0, 0.0009741878760440657, 0.0, 0.0, 0.0, 0.00349470571406284, 0.005158796184883671, 0.0, 0.002543590495715393, 0.0, 0.0, 0.0, 0.0024186733474197484, 0.0, 0.0, 0.0025728919479917614, 0.0012253054977474096, 0.0008659447787058363, 0.0, 0.0, 0.010393520398533996, 0.0030328006772634987, 0.020667753003066437, 0.0, 0.10240994178576657, 0.003792292895807176, 0.02477414813709958, 0.0012618052489713613, 0.0012305531065819786, 0.007558122990819089, 0.0, 0.0, 0.039956386675349634, 0.00581759103528232, 0.0, 0.0, 0.0009270255161870574, 0.0, 0.0, 0.0035970013884703963, 0.023291163385807005, 0.0032480366440550577, 0.0, 0.00036663472933746766, 0.004574662559505975, 0.0, 0.0384702884380079, 0.0012989171680587543, 0.0, 0.0, 0.0, 0.016209072072892766, 0.004132084732271613, 0.023222400580681465, 0.0, 0.0, 0.0022267151452435795, 0.0, 0.012713821438347372, 0.0003158193734519133, 0.0003156459551751127, 0.0, 0.005278139603540335, 0.007929042191106482, 0.0, 0.0, 0.008802828816020314]\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "score= accuracy_score(y_test, y_pred) * 100\n",
    "print(score)\n",
    "report_dt=classification_report(y_test, y_pred)\n",
    "print(report_dt)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "pyplot.bar(range(len(dt.feature_importances_)), dt.feature_importances_)\n",
    "pyplot.show()\n",
    "print(list(dt.feature_importances_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.21212121212122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98       552\n",
      "         1.0       0.93      0.83      0.88       108\n",
      "\n",
      "    accuracy                           0.96       660\n",
      "   macro avg       0.95      0.91      0.93       660\n",
      "weighted avg       0.96      0.96      0.96       660\n",
      "\n",
      "[[545   7]\n",
      " [ 18  90]]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.109863\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                    168   No. Observations:                 6599\n",
      "Model:                          Logit   Df Residuals:                     6433\n",
      "Method:                           MLE   Df Model:                          165\n",
      "Date:                Sun, 15 Mar 2020   Pseudo R-squ.:                  0.7444\n",
      "Time:                        19:39:48   Log-Likelihood:                -724.99\n",
      "converged:                       True   LL-Null:                       -2836.1\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "2             -0.0968      0.022     -4.440      0.000      -0.140      -0.054\n",
      "3              0.0188      0.006      3.189      0.001       0.007       0.030\n",
      "4             -0.1008      0.022     -4.481      0.000      -0.145      -0.057\n",
      "5             -0.0202      0.007     -3.006      0.003      -0.033      -0.007\n",
      "6              0.0012      0.002      0.546      0.585      -0.003       0.005\n",
      "7             -0.0143      0.005     -3.038      0.002      -0.024      -0.005\n",
      "8             -0.0019      0.020     -0.097      0.923      -0.041       0.037\n",
      "9              0.0103      0.004      2.780      0.005       0.003       0.018\n",
      "10            -0.0431      0.011     -3.910      0.000      -0.065      -0.021\n",
      "11            -0.0367      0.008     -4.408      0.000      -0.053      -0.020\n",
      "12             0.0097      0.007      1.328      0.184      -0.005       0.024\n",
      "13            -0.0416      0.018     -2.328      0.020      -0.077      -0.007\n",
      "14            -0.0067      0.009     -0.770      0.441      -0.024       0.010\n",
      "15            -0.0404      0.016     -2.468      0.014      -0.072      -0.008\n",
      "16            -0.0220      0.010     -2.121      0.034      -0.042      -0.002\n",
      "17            -0.0074      0.003     -2.551      0.011      -0.013      -0.002\n",
      "18             0.0379      0.005      7.131      0.000       0.027       0.048\n",
      "19            -0.0039      0.005     -0.821      0.412      -0.013       0.005\n",
      "20            -0.0016      0.010     -0.151      0.880      -0.022       0.019\n",
      "21            -0.0015      0.005     -0.298      0.766      -0.011       0.008\n",
      "22             0.0023      0.009      0.254      0.800      -0.016       0.020\n",
      "23            -0.0433      0.025     -1.747      0.081      -0.092       0.005\n",
      "24             0.0064      0.013      0.505      0.614      -0.018       0.031\n",
      "25            -0.0018      0.002     -0.932      0.351      -0.006       0.002\n",
      "26            -0.0203      0.007     -2.770      0.006      -0.035      -0.006\n",
      "27            -0.0290      0.007     -3.971      0.000      -0.043      -0.015\n",
      "28             0.0198      0.012      1.649      0.099      -0.004       0.043\n",
      "29            -0.0081      0.004     -2.206      0.027      -0.015      -0.001\n",
      "30            -0.0338      0.011     -3.048      0.002      -0.056      -0.012\n",
      "31            -0.0122      0.011     -1.062      0.288      -0.035       0.010\n",
      "32            -0.0101      0.005     -1.860      0.063      -0.021       0.001\n",
      "33            -0.0215      0.007     -3.098      0.002      -0.035      -0.008\n",
      "34            -0.0619      0.013     -4.637      0.000      -0.088      -0.036\n",
      "35            -0.0048      0.004     -1.083      0.279      -0.013       0.004\n",
      "36            -0.0449      0.012     -3.704      0.000      -0.069      -0.021\n",
      "37            -0.0295      0.011     -2.617      0.009      -0.052      -0.007\n",
      "38            -0.0083      0.003     -2.715      0.007      -0.014      -0.002\n",
      "39             0.0247      0.013      1.944      0.052      -0.000       0.050\n",
      "40            -0.0082      0.008     -1.049      0.294      -0.023       0.007\n",
      "41            -0.0205      0.025     -0.825      0.409      -0.069       0.028\n",
      "42             0.0462      0.022      2.100      0.036       0.003       0.089\n",
      "43            -0.0226      0.006     -3.770      0.000      -0.034      -0.011\n",
      "44             0.0326      0.010      3.130      0.002       0.012       0.053\n",
      "45             0.0011      0.013      0.084      0.933      -0.025       0.027\n",
      "46            -0.0278      0.020     -1.357      0.175      -0.068       0.012\n",
      "47             0.0007      0.004      0.189      0.850      -0.007       0.008\n",
      "48            -0.0131      0.004     -3.628      0.000      -0.020      -0.006\n",
      "49             0.0114      0.008      1.469      0.142      -0.004       0.027\n",
      "50            -0.0114      0.006     -1.836      0.066      -0.024       0.001\n",
      "51             0.0257      0.007      3.472      0.001       0.011       0.040\n",
      "52             0.0343      0.012      2.933      0.003       0.011       0.057\n",
      "53             0.0680      0.018      3.881      0.000       0.034       0.102\n",
      "54             0.0424      0.016      2.616      0.009       0.011       0.074\n",
      "55            -0.0002      0.002     -0.085      0.933      -0.004       0.004\n",
      "56            -0.0002      0.005     -0.048      0.961      -0.010       0.009\n",
      "57             0.0095      0.004      2.225      0.026       0.001       0.018\n",
      "58            -0.0460      0.019     -2.372      0.018      -0.084      -0.008\n",
      "59            -0.0209      0.005     -4.369      0.000      -0.030      -0.012\n",
      "60             0.0379      0.010      3.810      0.000       0.018       0.057\n",
      "61            -0.0100      0.010     -1.026      0.305      -0.029       0.009\n",
      "62            -0.0085      0.004     -2.062      0.039      -0.017      -0.000\n",
      "63            -0.0041      0.005     -0.752      0.452      -0.015       0.007\n",
      "64             0.0107      0.015      0.729      0.466      -0.018       0.039\n",
      "65             0.0032      0.006      0.538      0.590      -0.009       0.015\n",
      "66            -0.0141      0.010     -1.381      0.167      -0.034       0.006\n",
      "67             0.0804      0.021      3.775      0.000       0.039       0.122\n",
      "68            -0.0148      0.007     -2.054      0.040      -0.029      -0.001\n",
      "69             0.0080      0.021      0.386      0.699      -0.032       0.048\n",
      "70            -0.0021      0.003     -0.611      0.541      -0.009       0.005\n",
      "71             0.0185      0.026      0.722      0.470      -0.032       0.069\n",
      "72             0.0658      0.028      2.359      0.018       0.011       0.120\n",
      "73            -0.0416      0.012     -3.534      0.000      -0.065      -0.019\n",
      "74            -0.0180      0.008     -2.389      0.017      -0.033      -0.003\n",
      "75             0.0227      0.018      1.286      0.198      -0.012       0.057\n",
      "76            -0.0172      0.008     -2.195      0.028      -0.032      -0.002\n",
      "77             0.0225      0.010      2.285      0.022       0.003       0.042\n",
      "78            -0.0048      0.008     -0.612      0.540      -0.020       0.011\n",
      "79             0.0254      0.008      3.129      0.002       0.009       0.041\n",
      "80             0.0206      0.017      1.237      0.216      -0.012       0.053\n",
      "81             0.0020      0.005      0.430      0.667      -0.007       0.011\n",
      "82            -0.0265      0.011     -2.518      0.012      -0.047      -0.006\n",
      "83            -0.0044      0.027     -0.164      0.870      -0.057       0.048\n",
      "84            -0.0230      0.010     -2.389      0.017      -0.042      -0.004\n",
      "85             0.0057      0.003      1.859      0.063      -0.000       0.012\n",
      "86             0.0339      0.008      4.126      0.000       0.018       0.050\n",
      "87            -0.0253      0.012     -2.128      0.033      -0.049      -0.002\n",
      "88             0.0328      0.009      3.672      0.000       0.015       0.050\n",
      "89            -0.0090      0.003     -3.077      0.002      -0.015      -0.003\n",
      "90             0.0100      0.009      1.117      0.264      -0.008       0.028\n",
      "91             0.0083      0.007      1.161      0.246      -0.006       0.022\n",
      "92            -0.0015      0.003     -0.478      0.633      -0.007       0.005\n",
      "93             0.0153      0.012      1.304      0.192      -0.008       0.038\n",
      "94             0.0329      0.014      2.303      0.021       0.005       0.061\n",
      "95             0.0160      0.004      3.663      0.000       0.007       0.025\n",
      "96             0.0001      0.012      0.010      0.992      -0.024       0.024\n",
      "97            -0.0165      0.004     -4.419      0.000      -0.024      -0.009\n",
      "98             0.0028      0.004      0.785      0.433      -0.004       0.010\n",
      "99            -0.0498      0.011     -4.666      0.000      -0.071      -0.029\n",
      "100            0.1060      0.016      6.645      0.000       0.075       0.137\n",
      "101           -0.0161      0.022     -0.733      0.463      -0.059       0.027\n",
      "102           -0.0517      0.013     -4.003      0.000      -0.077      -0.026\n",
      "103           -0.0148      0.012     -1.231      0.218      -0.038       0.009\n",
      "104            0.0269      0.010      2.735      0.006       0.008       0.046\n",
      "105           -0.0185      0.013     -1.397      0.163      -0.045       0.007\n",
      "106            0.0284      0.017      1.701      0.089      -0.004       0.061\n",
      "107           -0.0032      0.004     -0.839      0.402      -0.011       0.004\n",
      "108           -0.0139      0.003     -4.639      0.000      -0.020      -0.008\n",
      "109            0.0117      0.005      2.547      0.011       0.003       0.021\n",
      "110            0.0083      0.007      1.205      0.228      -0.005       0.022\n",
      "111            0.0079      0.015      0.527      0.598      -0.021       0.037\n",
      "112           -0.0101      0.008     -1.332      0.183      -0.025       0.005\n",
      "113           -0.0057      0.012     -0.456      0.649      -0.030       0.019\n",
      "114            0.0246      0.016      1.573      0.116      -0.006       0.055\n",
      "115            0.0061      0.019      0.314      0.754      -0.032       0.044\n",
      "116           -0.0166      0.005     -3.227      0.001      -0.027      -0.007\n",
      "117            0.0104      0.003      3.549      0.000       0.005       0.016\n",
      "118            0.0091      0.005      1.931      0.053      -0.000       0.018\n",
      "119            0.0269      0.006      4.414      0.000       0.015       0.039\n",
      "120            0.0767      0.014      5.650      0.000       0.050       0.103\n",
      "121           -0.0218      0.007     -3.210      0.001      -0.035      -0.008\n",
      "122            0.0206      0.010      2.050      0.040       0.001       0.040\n",
      "123            0.0191      0.006      2.949      0.003       0.006       0.032\n",
      "124           -0.0262      0.008     -3.176      0.001      -0.042      -0.010\n",
      "125           -0.0086      0.006     -1.494      0.135      -0.020       0.003\n",
      "126           -0.0213      0.005     -4.031      0.000      -0.032      -0.011\n",
      "127            0.0381      0.011      3.607      0.000       0.017       0.059\n",
      "128           -0.0192      0.009     -2.200      0.028      -0.036      -0.002\n",
      "129            0.0139      0.010      1.444      0.149      -0.005       0.033\n",
      "130            0.0143      0.007      2.113      0.035       0.001       0.028\n",
      "131            0.0250      0.008      3.210      0.001       0.010       0.040\n",
      "132           -0.0143      0.009     -1.595      0.111      -0.032       0.003\n",
      "133           -0.0041      0.004     -1.131      0.258      -0.011       0.003\n",
      "134           -0.0111      0.008     -1.409      0.159      -0.026       0.004\n",
      "135            0.0224      0.012      1.886      0.059      -0.001       0.046\n",
      "136           -0.0048      0.010     -0.472      0.637      -0.025       0.015\n",
      "137           -0.0210      0.011     -1.944      0.052      -0.042       0.000\n",
      "138           -0.0108      0.007     -1.510      0.131      -0.025       0.003\n",
      "139           -0.0180      0.005     -3.665      0.000      -0.028      -0.008\n",
      "140           -0.0283      0.010     -2.761      0.006      -0.048      -0.008\n",
      "141            0.0166      0.007      2.452      0.014       0.003       0.030\n",
      "142            0.0065      0.008      0.779      0.436      -0.010       0.023\n",
      "143        -1.961e-05      0.020     -0.001      0.999      -0.039       0.039\n",
      "144           -0.0216      0.020     -1.101      0.271      -0.060       0.017\n",
      "145            0.0623      0.015      4.025      0.000       0.032       0.093\n",
      "146           -0.0014      0.005     -0.271      0.786      -0.011       0.009\n",
      "147            0.0055      0.005      1.159      0.246      -0.004       0.015\n",
      "148           -0.0128      0.007     -1.753      0.080      -0.027       0.002\n",
      "149            0.0083      0.021      0.387      0.699      -0.034       0.050\n",
      "150           -0.0522      0.015     -3.584      0.000      -0.081      -0.024\n",
      "151            0.0168      0.019      0.885      0.376      -0.020       0.054\n",
      "152            0.0452      0.017      2.689      0.007       0.012       0.078\n",
      "153            0.0132      0.014      0.922      0.357      -0.015       0.041\n",
      "154           -0.0050      0.021     -0.236      0.813      -0.046       0.036\n",
      "155           -0.0106      0.012     -0.916      0.360      -0.033       0.012\n",
      "156            0.0231      0.008      3.029      0.002       0.008       0.038\n",
      "157            0.0263      0.009      3.044      0.002       0.009       0.043\n",
      "158           -0.0019      0.002     -0.978      0.328      -0.006       0.002\n",
      "159           -0.0050      0.002     -2.493      0.013      -0.009      -0.001\n",
      "160            0.0033      0.003      1.107      0.268      -0.003       0.009\n",
      "161            0.0049      0.006      0.759      0.448      -0.008       0.017\n",
      "162            0.0032      0.004      0.764      0.445      -0.005       0.012\n",
      "163           -0.0214      0.010     -2.211      0.027      -0.040      -0.002\n",
      "164           -0.0405      0.009     -4.335      0.000      -0.059      -0.022\n",
      "165           -0.0057      0.008     -0.708      0.479      -0.021       0.010\n",
      "166           -0.0188      0.009     -2.040      0.041      -0.037      -0.001\n",
      "167            0.0068      0.004      1.873      0.061      -0.000       0.014\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.21 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "logreg=LogisticRegression(solver='liblinear',multi_class='ovr')\n",
    "logreg.fit(X_train,y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "score= accuracy_score(y_test, y_pred) * 100\n",
    "print(score)\n",
    "report_logreg=classification_report(y_test, y_pred)\n",
    "print(report_logreg)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "import statsmodels.api as sm\n",
    "logit_model=sm.Logit(y,X)\n",
    "result=logit_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.33333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99       552\n",
      "         1.0       1.00      0.90      0.95       108\n",
      "\n",
      "    accuracy                           0.98       660\n",
      "   macro avg       0.99      0.95      0.97       660\n",
      "weighted avg       0.98      0.98      0.98       660\n",
      "\n",
      "[[552   0]\n",
      " [ 11  97]]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs = -1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "score= accuracy_score(y_test, y_pred) * 100\n",
    "print(score)\n",
    "report_rf=classification_report(y_test, y_pred)\n",
    "print(report_rf)\n",
    "print(confusion_matrix(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.6060606060606\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      1.00      0.95       552\n",
      "         1.0       1.00      0.43      0.60       108\n",
      "\n",
      "    accuracy                           0.91       660\n",
      "   macro avg       0.95      0.71      0.77       660\n",
      "weighted avg       0.92      0.91      0.89       660\n",
      "\n",
      "[[552   0]\n",
      " [ 62  46]]\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(gamma='auto')\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "score= accuracy_score(y_test, y_pred) * 100\n",
    "print(score)\n",
    "report_svm=classification_report(y_test, y_pred)\n",
    "print(report_svm)\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.81818181818181\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      0.99      0.98       552\n",
      "         1.0       0.96      0.84      0.90       108\n",
      "\n",
      "    accuracy                           0.97       660\n",
      "   macro avg       0.96      0.92      0.94       660\n",
      "weighted avg       0.97      0.97      0.97       660\n",
      "\n",
      "[[548   4]\n",
      " [ 17  91]]\n"
     ]
    }
   ],
   "source": [
    "xgb = xgb.XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "score= accuracy_score(y_test, y_pred,) * 100\n",
    "print(score)\n",
    "report_xgb=classification_report(y_test, y_pred)\n",
    "print(report_xgb)\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS VALIDATIONS / 10 - FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION_TREE: 0.8051413988136294\n",
      "LOGISTIC_REGRESSION: 0.8107442405849083\n",
      "SUPPORT_VECTOR_MACHINES: 0.8498252632547019\n",
      "RANDOM_FOREST: 0.7809054122407689\n",
      "XGBOOST: 0.8219545224628686\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "print(\"DECISION_TREE:\" ,cross_val_score(DecisionTreeClassifier(), X, y, cv=10, scoring =\"accuracy\").mean())\n",
    "print(\"LOGISTIC_REGRESSION:\" ,cross_val_score(LogisticRegression(solver='liblinear',multi_class='ovr'), X, y,cv=10, scoring = \"accuracy\").mean())\n",
    "print(\"SUPPORT_VECTOR_MACHINES:\" ,cross_val_score(SVC(gamma='auto'), X, y, cv=10, scoring =\"accuracy\").mean())\n",
    "print(\"RANDOM_FOREST:\" ,cross_val_score(RandomForestClassifier(n_estimators=40), X, y, cv=10, scoring =\"accuracy\").mean())\n",
    "print(\"XGBOOST:\" ,cross_val_score(xgb.XGBClassifier(), X, y, cv=10, scoring =\"accuracy\").mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
